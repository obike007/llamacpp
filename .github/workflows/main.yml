name: CI/CD Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  IMAGE_NAME: llamacpp-server

jobs:
  build-and-push:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v2

    - name: Login to Docker Hub
      uses: docker/login-action@v2
      with:
        username: ${{ secrets.DOCKER_HUB_USERNAME }}
        password: ${{ secrets.DOCKER_HUB_TOKEN }}

    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: |
          ${{ secrets.DOCKER_HUB_USERNAME }}/${{ env.IMAGE_NAME }}:${{ github.sha }}
          ${{ secrets.DOCKER_HUB_USERNAME }}/${{ env.IMAGE_NAME }}:latest
        cache-from: type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME }}/${{ env.IMAGE_NAME }}:buildcache
        cache-to: type=registry,ref=${{ secrets.DOCKER_HUB_USERNAME }}/${{ env.IMAGE_NAME }}:buildcache,mode=max

  deploy-to-kind:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Create kind config file
      run: |
        cat > kind-config.yaml <<EOF
        kind: Cluster
        apiVersion: kind.x-k8s.io/v1alpha4
        nodes:
        - role: control-plane
          extraPortMappings:
          - containerPort: 30083
            hostPort: 8084
          - containerPort: 30090
            hostPort: 9090
          - containerPort: 30030
            hostPort: 3001
        EOF

    - name: Verify config file
      run: |
        echo "Generated kind config:"
        cat kind-config.yaml

    - name: Create kind cluster
      uses: helm/kind-action@v1.5.0
      with:
        cluster_name: llama-cluster
        config: kind-config.yaml
        version: v0.17.0
        wait: 60s
        verbosity: 0
        kubectl_version: v1.23.12

    # NEW OPTIMIZED DEPLOYMENT STEPS
    - name: Setup Kubernetes Infrastructure
      run: |
        echo "Setting up namespace and storage..."
        
        # Apply namespace
        kubectl apply -f kubernetes/namespace.yaml
        
        # Create PVC for model storage
        kubectl apply -f - <<EOF
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: model-storage
          namespace: llama-app
          labels:
            app: llamacpp
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
        EOF
        
        echo "âœ… Infrastructure setup complete"

    - name: Download Model
      timeout-minutes: 20
      run: |
        # Create unique job name
        JOB_NAME="model-download-${{ github.run_number }}"
        
        echo "Creating model download job: $JOB_NAME"
        
        kubectl apply -f - <<EOF
        apiVersion: batch/v1
        kind: Job
        metadata:
          name: $JOB_NAME
          namespace: llama-app
          labels:
            app: llamacpp
            job-type: model-download
            run-id: "${{ github.run_number }}"
        spec:
          ttlSecondsAfterFinished: 300
          template:
            spec:
              restartPolicy: Never
              containers:
              - name: model-downloader
                image: curlimages/curl:latest
                resources:
                  requests:
                    memory: "128Mi"
                    cpu: "100m"
                  limits:
                    memory: "256Mi"
                    cpu: "500m"
                command: ['sh', '-c']
                args:
                - |
                  set -e
                  MODEL_FILE="/models/model.gguf"
                  MODEL_URL="https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf"
                  
                  echo "=== Model Download Job Started ==="
                  echo "Job: $JOB_NAME"
                  echo "Target: \$MODEL_FILE" 
                  echo "Time: \$(date)"
                  
                  if [ -f "\$MODEL_FILE" ] && [ -s "\$MODEL_FILE" ]; then
                    SIZE=\$(ls -lh "\$MODEL_FILE" | awk '{print \$5}')
                    echo "âœ… Model exists: \$SIZE - Skipping download"
                    exit 0
                  fi
                  
                  echo "ðŸ“¥ Downloading model (~4GB, may take 5-15 minutes)..."
                  curl -L --fail --show-error --progress-bar \
                       --connect-timeout 30 --max-time 1200 --retry 3 \
                       -C - -o "\$MODEL_FILE.tmp" "\$MODEL_URL"
                  
                  mv "\$MODEL_FILE.tmp" "\$MODEL_FILE"
                  
                  if [ ! -s "\$MODEL_FILE" ]; then
                    echo "âŒ ERROR: Download failed!"
                    exit 1
                  fi
                  
                  echo "âœ… Download complete: \$(ls -lh "\$MODEL_FILE" | awk '{print \$5}')"
                volumeMounts:
                - name: model-storage
                  mountPath: /models
              volumes:
              - name: model-storage
                persistentVolumeClaim:
                  claimName: model-storage
          backoffLimit: 2
        EOF
        
        echo "â³ Waiting for model download..."
        kubectl wait --for=condition=complete --timeout=1200s job/$JOB_NAME -n llama-app || {
          echo "âŒ Download failed - showing logs:"
          kubectl logs job/$JOB_NAME -n llama-app
          kubectl describe job $JOB_NAME -n llama-app
          exit 1
        }
        
        echo "âœ… Model ready!"
        kubectl logs job/$JOB_NAME -n llama-app --tail=5

    - name: Deploy Application
      timeout-minutes: 5
      run: |
        echo "ðŸš€ Deploying llamacpp application..."
        
        # Update image references in your existing deployment
        sed -i "s|IMAGE_TAG|${{ github.sha }}|g" kubernetes/deployment.yaml
        sed -i "s|DOCKER_HUB_USERNAME|${{ secrets.DOCKER_HUB_USERNAME }}|g" kubernetes/deployment.yaml
        
        # Temporarily remove init container section since model is pre-downloaded
        echo "Creating optimized deployment without init container..."
        
        # Use sed to remove the initContainers section
        sed -i '/initContainers:/,/^      containers:/{ /^      containers:/!d; }' kubernetes/deployment.yaml
        
        # Show what we're deploying
        echo "ðŸ“‹ Deployment preview:"
        head -20 kubernetes/deployment.yaml
        
        # Apply manifests using your existing files
        kubectl apply -f kubernetes/configmap.yaml
        kubectl apply -f kubernetes/deployment.yaml
        kubectl apply -f kubernetes/service.yaml
        
        # Wait for deployment
        echo "â³ Waiting for deployment..."
        kubectl wait --for=condition=available --timeout=300s deployment/llamacpp -n llama-app || {
          echo "âŒ Deployment failed:"
          kubectl get pods -n llama-app -o wide
          kubectl describe pods -n llama-app -l app=llamacpp
          kubectl logs -n llama-app -l app=llamacpp --tail=50
          exit 1
        }
        
        echo "âœ… Deployment successful!"
        kubectl get pods -n llama-app -o wide
        
        # Wait for deployment
        echo "â³ Waiting for deployment..."
        kubectl wait --for=condition=available --timeout=300s deployment/llamacpp -n llama-app || {
          echo "âŒ Deployment failed:"
          kubectl get pods -n llama-app -o wide
          kubectl describe pods -n llama-app -l app=llamacpp
          kubectl logs -n llama-app -l app=llamacpp --tail=50
          exit 1
        }
        
        echo "âœ… Deployment successful!"
        kubectl get pods -n llama-app -o wide

    - name: Deploy monitoring stack
      run: |
        echo "ðŸ“Š Setting up monitoring..."
        bash scripts/setup-monitoring.sh || echo "âš ï¸  Monitoring setup failed, continuing..."

    - name: Cleanup old jobs
      if: always()
      run: |
        echo "ðŸ§¹ Cleaning up old download jobs..."
        # Keep only 3 most recent model download jobs
        kubectl get jobs -n llama-app -l job-type=model-download \
          --sort-by=.metadata.creationTimestamp \
          -o jsonpath='{range .items[:-3]}{.metadata.name}{"\n"}{end}' \
        | xargs -r kubectl delete job -n llama-app || echo "No jobs to clean"
        echo "âœ… Cleanup completed"

    - name: Show final status
      if: always()
      run: |
        echo "ðŸ” Final deployment status:"
        kubectl get all -n llama-app
        echo ""
        echo "ðŸ“‹ Application endpoints:"
        kubectl get svc -n llama-app
        echo ""
        echo "ðŸ’¾ Storage status:"
        kubectl get pvc -n llama-app